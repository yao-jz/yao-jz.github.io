---
title: "Workshop of Multimodal Machine Learning + MoE, Tsinghua University"
collection: research
type: 'Workshop'
excerpt: ''
permalink: /research/2022-06-01-zhouworkshop
date: 2022-06-01
year: 2022
---

##

The main topic of this workshop is Multimodal Machine Learning. At the same time, inspired by the [Language-Image Mixture of Experts](https://arxiv.org/abs/2206.02770) (also refer to [this Google Blog](https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html)) proposed by Google recently, I am quite interested in the combination of Mixture of Experts architecture and Multimodal Information. After my literature research, I wrote a research proposal and designed experiments for the Multitask Multimodal Mixture of Experts approach.

### Multimodal Machine Learning

Apart from massive text data, there is also a large amount of multimodal data, like

### MMMoE: Advancing Multimodal Mixture-of-Experts Architecture to Power Next-Generation Multimodal Paradigm
