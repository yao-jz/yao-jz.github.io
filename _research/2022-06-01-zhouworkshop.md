---
title: "Workshop of Multimodal Machine Learning + MoE, Tsinghua University<img src='/images/logos/tsinghua.png' height="50" width="50">"
collection: research
type: 'Workshop'
excerpt: ''
permalink: /research/2022-06-01-zhouworkshop
date: 2022-06-01
year: 2022
---

The workshop is about Multimodal Machine Learning. At the same time, inspired by the [Language-Image Mixture of Experts](https://arxiv.org/abs/2206.02770) (also refer to [this Google Blog](https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html)) proposed by Google recently, I am quite interested in the combination of Mixture of Experts architecture and Multimodal Information. After my literature research, I wrote a research proposal and designed experiments for the Multitask Multimodal Mixture of Experts approach.

### A Hierarchical Overview of Multimodal Machine Learning --- And potential ideas with personal insights

Apart from massive text data, there is also a large amount of multimodal data, like audio, image, video, time-series, force sensors, tables, and so on. In this survey paper, [Multimodal Machine Learning: A Survey and Taxonomy](https://ieeexplore.ieee.org/abstract/document/8269806), the author identify and explore five core technical challenges: **Representation**, **Translation**, **Alignment**, **Fusion**, and **Co-learning**. Based on this, I did a literature survey, summarized the classical work, brainstormed on new possibilities, and gave a presentation (my slide is shared [**HERE**](<https://yao-jz.github.io/files/workshop/multimodal.pdf>)).

### MMMoE: Advancing Multimodal Mixture-of-Experts Architecture to Power Next-Generation Multimodal Paradigm

As mentioned above, the multimodal MoE is an exciting new field full of opportunity, which is under-investigated. So based on my knowledge and recent work, I wrote a research proposal and designed experiments for a MultiTask Multimodal Mixture of Experts approach, which I call it MMMoE (Triple-MoE). I hope this preliminary, untested idea can be implemented in the future and can inspire others in the interesting research of Multimodal MoEs. My research proposal can be found [**HERE**](<https://yao-jz.github.io/files/workshop/MMMoE-proposal.pdf>), and my slides for introduction can be found [**HERE**](<https://yao-jz.github.io/files/workshop/MMMoE_intro.pdf>)
