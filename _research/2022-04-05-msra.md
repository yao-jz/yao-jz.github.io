---
title: "Natural Langugae Computing Group, Microsoft Research Asia<img src='/images/logos/msra.jpeg' height="50" width="50">"
collection: research
type: 'Research Intern'
excerpt: ''
permalink: /research/2022-04-05-msra
date: 2022-04-05
---

In the Natural Language Computing Group, I get familiar with the process and core motivations of doing research in Companies like Microsoft. Their support and enthusiasm for research enrich my existing knowledge by a large step and broaden my horizons. I am deeply impressed by their ability to capture hot topics in academic frontiers and I have gained a lot.

During my research internship, I keep the habit of taking [reading notes](https://docs.google.com/document/d/16uHnH7Y9P7vNmDuI8bdeHlmi_CMgokgePyY2DTInJd8/edit?usp=sharing) for every paper I read. And I also keep [my research log](https://docs.google.com/document/d/1MoaPQMQx5kj11F6MRZg99dwlQqGFX92LC3gAk2HPE8A/edit?usp=sharing) (A Chinese Version) in Google Doc.

### Mixture of Experts Language Model Profiling

Due to the large amount of cost of all2all computation, the inference speed is a major obstacle in downstream applications. So I conduct a model profiling experiment of a Language Modeling MoE. I found that as the number of experts increases, the all2all computation takes a larger part of the computation. So maybe doing expert pruning or working on expert distribution technique is necessary.

### Mixture of Experts -- expert pruning experiment

After my exploration, I conduct an expert pruning research on my own, and the approach + results can be found in my research log presented above.
